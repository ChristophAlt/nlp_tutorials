{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SCRIPT = \"../datasets/semeval_2010_task_8.py\"\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "HEAD_START_MARKER = \"[HEAD]\"\n",
    "HEAD_END_MARKER = \"[/HEAD]\"\n",
    "TAIL_START_MARKER = \"[TAIL]\"\n",
    "TAIL_END_MARKER = \"[/TAIL]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_arguments(examples,\n",
    "                   tokenizer,\n",
    "                   max_length,\n",
    "                   truncation = \"longest_first\",\n",
    "                   argument_marker_mode= \"mark\",\n",
    "                   append_separator = \"[SEP]\",\n",
    "                   head_start_marker = \"[HEAD]\",\n",
    "                   head_end_marker = \"[/HEAD]\",\n",
    "                   tail_start_marker = \"[TAIL]\",\n",
    "                   tail_end_marker = \"[/TAIL]\"):\n",
    "    marked_tokens = []\n",
    "    for tokens, head_start, head_end, tail_start, tail_end in zip(\n",
    "        examples[\"tokens\"],\n",
    "        examples[\"head_start\"],\n",
    "        examples[\"head_end\"],\n",
    "        examples[\"tail_start\"],\n",
    "        examples[\"tail_end\"],\n",
    "    ):\n",
    "\n",
    "        head = (head_start, head_end, head_start_marker, head_end_marker)\n",
    "        tail = (tail_start, tail_end, tail_start_marker, tail_end_marker)\n",
    "\n",
    "        head_first = head_start < tail_start\n",
    "        first, second = (head, tail) if head_first else (tail, head)\n",
    "\n",
    "        first_start, first_end, first_start_marker, first_end_marker = first\n",
    "        second_start, second_end, second_start_marker, second_end_marker = second\n",
    "\n",
    "        first_tokens = tokens[first_start:first_end]\n",
    "        second_tokens = tokens[second_start:second_end]\n",
    "\n",
    "        marked_tokens.append(\n",
    "            tokens[:first_start]\n",
    "            + [first_start_marker]\n",
    "            + first_tokens\n",
    "            + [first_end_marker]\n",
    "            + tokens[first_end:second_start]\n",
    "            + [second_start_marker]\n",
    "            + second_tokens\n",
    "            + [second_end_marker]\n",
    "            + tokens[second_end:]\n",
    "        )\n",
    "\n",
    "    return tokenizer(\n",
    "        text=marked_tokens,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=truncation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sem_eval2010_task8 (/home/christoph/.cache/huggingface/datasets/sem_eval2010_task8/default/1.0.0/aa27127323f78f643e734263370540be63f5f10ba9545fa11e66eea7a7c671d4)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.DatasetDict(train=datasets.load_dataset(path=DATASET_SCRIPT, split=\"train\"))\n",
    "\n",
    "dataset.rename_column_(original_column_name=\"label\", new_column_name=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'head_start', 'head_end', 'tail_start', 'tail_end', 'labels'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head_end': 13,\n",
       " 'head_start': 12,\n",
       " 'labels': 3,\n",
       " 'tail_end': 16,\n",
       " 'tail_start': 15,\n",
       " 'tokens': ['The',\n",
       "  'system',\n",
       "  'as',\n",
       "  'described',\n",
       "  'above',\n",
       "  'has',\n",
       "  'its',\n",
       "  'greatest',\n",
       "  'application',\n",
       "  'in',\n",
       "  'an',\n",
       "  'arrayed',\n",
       "  'configuration',\n",
       "  'of',\n",
       "  'antenna',\n",
       "  'elements',\n",
       "  '.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"additional_special_tokens\": [\n",
    "            HEAD_START_MARKER,\n",
    "            HEAD_END_MARKER,\n",
    "            TAIL_START_MARKER,\n",
    "            TAIL_END_MARKER\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '##de', '##f', '##g', 'lives', 'in', 'y', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"ABCDEFG lives in Y.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 5925, 3207, 2546, 2290, 3268, 1999, 1061, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_output = tokenizer(\"ABCDEFG lives in Y.\")\n",
    "tokenizer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'abc', '##de', '##f', '##g', 'lives', 'in', 'y', '.', '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer_output[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/christoph/.cache/huggingface/datasets/sem_eval2010_task8/default/1.0.0/aa27127323f78f643e734263370540be63f5f10ba9545fa11e66eea7a7c671d4/cache-dc8d8bb83a31d46a.arrow\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return mark_arguments(examples, tokenizer=tokenizer, max_length=64)\n",
    "\n",
    "preprocessed_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'head_end', 'head_start', 'input_ids', 'labels', 'tail_end', 'tail_start', 'token_type_ids', 'tokens'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = preprocessed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'system',\n",
       " 'as',\n",
       " 'described',\n",
       " 'above',\n",
       " 'has',\n",
       " 'its',\n",
       " 'greatest',\n",
       " 'application',\n",
       " 'in',\n",
       " 'an',\n",
       " 'array',\n",
       " '##ed',\n",
       " '[HEAD]',\n",
       " 'configuration',\n",
       " '[/HEAD]',\n",
       " 'of',\n",
       " 'antenna',\n",
       " '[TAIL]',\n",
       " 'elements',\n",
       " '[/TAIL]',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset.set_format(\"pt\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christoph/.miniconda3/envs/tutorial/lib/python3.7/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'input_ids': tensor([  101,  1996,  2291,  2004,  2649,  2682,  2038,  2049,  4602,  4646,\n",
       "          1999,  2019,  9140,  2098, 30522,  9563, 30523,  1997, 13438, 30524,\n",
       "          3787, 30525,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'labels': tensor(3),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "def train_model(model, dataset, optimizer, num_epochs, cuda_device=-1, batch_size=2, log_every_n_batches=10):\n",
    "    device = torch.device(\"cuda\", cuda_device) if cuda_device > -1 else torch.device(\"cpu\")\n",
    "    \n",
    "    if \"validation\" in dataset:\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        validation_dataset = dataset[\"validation\"]\n",
    "    else:\n",
    "        split_train_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "        train_dataset = split_train_dataset[\"train\"]\n",
    "        validation_dataset = split_train_dataset[\"test\"]\n",
    "    \n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "        for batch_idx, train_batch in enumerate(train_dataloader, start=1):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_batch = {key: tensor.to(device) for key, tensor in train_batch.items()}\n",
    "            \n",
    "            output = model(**train_batch)\n",
    "            \n",
    "            loss = output.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx % log_every_n_batches) == 0:\n",
    "                print(f\"[{batch_idx}/{len(train_dataloader)}] train loss: {train_loss / batch_idx}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.\n",
    "        val_f1_score = datasets.load_metric(\"f1\")\n",
    "        with torch.no_grad():\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "            for validation_batch in validation_dataloader:\n",
    "                validation_batch = {key: tensor.to(device) for key, tensor in validation_batch.items()}\n",
    "                \n",
    "                output = model(**validation_batch)\n",
    "                \n",
    "                logits = output.logits\n",
    "                \n",
    "                pred_labels = logits.argmax(dim=-1)\n",
    "                true_labels = validation_batch[\"labels\"]\n",
    "                \n",
    "                val_f1_score.add_batch(predictions=pred_labels, references=true_labels)\n",
    "                val_loss += output.loss.item()\n",
    "            \n",
    "            print(\"val loss: \", val_loss / len(validation_dataloader))\n",
    "            print(\"val f1:\", val_f1_score.compute(average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = 0  # if you have a GPU, otherwise set to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached split indices for dataset at /home/christoph/.cache/huggingface/datasets/sem_eval2010_task8/default/1.0.0/aa27127323f78f643e734263370540be63f5f10ba9545fa11e66eea7a7c671d4/cache-20f8cec6287cca4b.arrow and /home/christoph/.cache/huggingface/datasets/sem_eval2010_task8/default/1.0.0/aa27127323f78f643e734263370540be63f5f10ba9545fa11e66eea7a7c671d4/cache-a3c40cd4a624c5a8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/450] train loss: 2.9193800687789917\n",
      "[20/450] train loss: 2.8977689146995544\n",
      "[30/450] train loss: 2.8806257009506226\n",
      "[40/450] train loss: 2.8622309446334837\n",
      "[50/450] train loss: 2.848922486305237\n",
      "[60/450] train loss: 2.8355388164520265\n",
      "[70/450] train loss: 2.8195022446768623\n",
      "[80/450] train loss: 2.8023325741291045\n",
      "[90/450] train loss: 2.789883515569899\n",
      "[100/450] train loss: 2.774534707069397\n",
      "[110/450] train loss: 2.7645813660188154\n",
      "[120/450] train loss: 2.7528449575106304\n",
      "[130/450] train loss: 2.7404092917075524\n",
      "[140/450] train loss: 2.7257826157978604\n",
      "[150/450] train loss: 2.7105794858932497\n",
      "[160/450] train loss: 2.701285018026829\n",
      "[170/450] train loss: 2.677043244417976\n",
      "[180/450] train loss: 2.6637300491333007\n",
      "[190/450] train loss: 2.6513121567274394\n",
      "[200/450] train loss: 2.635449838638306\n",
      "[210/450] train loss: 2.6247129213242304\n",
      "[220/450] train loss: 2.608935646577315\n",
      "[230/450] train loss: 2.5923818178798843\n",
      "[240/450] train loss: 2.5733977988362313\n",
      "[250/450] train loss: 2.556113686084747\n",
      "[260/450] train loss: 2.540228492480058\n",
      "[270/450] train loss: 2.5197600934240554\n",
      "[280/450] train loss: 2.502623049276216\n",
      "[290/450] train loss: 2.486356566692221\n",
      "[300/450] train loss: 2.46653693318367\n",
      "[310/450] train loss: 2.446563228868669\n",
      "[320/450] train loss: 2.4275746468454598\n",
      "[330/450] train loss: 2.409774020946387\n",
      "[340/450] train loss: 2.393317152822719\n",
      "[350/450] train loss: 2.3761713681902203\n",
      "[360/450] train loss: 2.3592861764960817\n",
      "[370/450] train loss: 2.3434037981806575\n",
      "[380/450] train loss: 2.3250528643005772\n",
      "[390/450] train loss: 2.3062752173497127\n",
      "[400/450] train loss: 2.2860175019502638\n",
      "[410/450] train loss: 2.2693451151615234\n",
      "[420/450] train loss: 2.25090180067789\n",
      "[430/450] train loss: 2.232319715965626\n",
      "[440/450] train loss: 2.215809887647629\n",
      "[450/450] train loss: 2.1978251588344575\n",
      "val loss:  1.4204768276214599\n",
      "val f1: {'f1': 0.62625}\n",
      "[10/450] train loss: 1.4504067540168761\n",
      "[20/450] train loss: 1.3643874853849411\n",
      "[30/450] train loss: 1.3443777680397033\n",
      "[40/450] train loss: 1.4015773415565491\n",
      "[50/450] train loss: 1.3987356567382812\n",
      "[60/450] train loss: 1.3946651697158814\n",
      "[70/450] train loss: 1.3778029935700553\n",
      "[80/450] train loss: 1.3763272970914842\n",
      "[90/450] train loss: 1.3560807784398397\n",
      "[100/450] train loss: 1.3513346672058106\n",
      "[110/450] train loss: 1.35551876263185\n",
      "[120/450] train loss: 1.3507104426622392\n",
      "[130/450] train loss: 1.342181466634457\n",
      "[140/450] train loss: 1.3296544964824404\n",
      "[150/450] train loss: 1.3153445967038473\n",
      "[160/450] train loss: 1.3175416894257068\n",
      "[170/450] train loss: 1.301453279397067\n",
      "[180/450] train loss: 1.2950996743308174\n",
      "[190/450] train loss: 1.2864008090997998\n",
      "[200/450] train loss: 1.2710618627071382\n",
      "[210/450] train loss: 1.2671233364513943\n",
      "[220/450] train loss: 1.2538172133944252\n",
      "[230/450] train loss: 1.2468344447405442\n",
      "[240/450] train loss: 1.2332724258303642\n",
      "[250/450] train loss: 1.2205041589736938\n",
      "[260/450] train loss: 1.2134421105568225\n",
      "[270/450] train loss: 1.2008003925835644\n",
      "[280/450] train loss: 1.1927629377160753\n",
      "[290/450] train loss: 1.1875394860218311\n",
      "[300/450] train loss: 1.1754235630234082\n",
      "[310/450] train loss: 1.1642636128971653\n",
      "[320/450] train loss: 1.1531419266946614\n",
      "[330/450] train loss: 1.146387615980524\n",
      "[340/450] train loss: 1.1399574494537186\n",
      "[350/450] train loss: 1.1312726050615312\n",
      "[360/450] train loss: 1.1241829874614875\n",
      "[370/450] train loss: 1.1142818020002263\n",
      "[380/450] train loss: 1.1052423722649876\n",
      "[390/450] train loss: 1.100590131450922\n",
      "[400/450] train loss: 1.091080114170909\n",
      "[410/450] train loss: 1.0857362915103028\n",
      "[420/450] train loss: 1.079201541982946\n",
      "[430/450] train loss: 1.0717336780110072\n",
      "[440/450] train loss: 1.0657237420705232\n",
      "[450/450] train loss: 1.0584015915791194\n",
      "val loss:  0.8157688909769059\n",
      "val f1: {'f1': 0.77375}\n",
      "[10/450] train loss: 0.7671195477247238\n",
      "[20/450] train loss: 0.6685091018676758\n",
      "[30/450] train loss: 0.6506736467281977\n",
      "[40/450] train loss: 0.6999856241047382\n",
      "[50/450] train loss: 0.6932553088665009\n",
      "[60/450] train loss: 0.6868080511689186\n",
      "[70/450] train loss: 0.6773274460009167\n",
      "[80/450] train loss: 0.6816272597759963\n",
      "[90/450] train loss: 0.6786657813522551\n",
      "[100/450] train loss: 0.6808258989453315\n",
      "[110/450] train loss: 0.6826351499015635\n",
      "[120/450] train loss: 0.6963969094057878\n",
      "[130/450] train loss: 0.6948306367947505\n",
      "[140/450] train loss: 0.6899255810039384\n",
      "[150/450] train loss: 0.6836864219109218\n",
      "[160/450] train loss: 0.6915139203891159\n",
      "[170/450] train loss: 0.6789570028291029\n",
      "[180/450] train loss: 0.6812816086742614\n",
      "[190/450] train loss: 0.6773041878875933\n",
      "[200/450] train loss: 0.6683754369616508\n",
      "[210/450] train loss: 0.6734409802016758\n",
      "[220/450] train loss: 0.6636709686030041\n",
      "[230/450] train loss: 0.6629171088985775\n",
      "[240/450] train loss: 0.6592100420345862\n",
      "[250/450] train loss: 0.6531909098625183\n",
      "[260/450] train loss: 0.6518251040807137\n",
      "[270/450] train loss: 0.6450501868018398\n",
      "[280/450] train loss: 0.6442091255315713\n",
      "[290/450] train loss: 0.6455349235699095\n",
      "[300/450] train loss: 0.6392010017236074\n",
      "[310/450] train loss: 0.6323835438297641\n",
      "[320/450] train loss: 0.6249504076782614\n",
      "[330/450] train loss: 0.6210817708662062\n",
      "[340/450] train loss: 0.6193147755282766\n",
      "[350/450] train loss: 0.6169017394099917\n",
      "[360/450] train loss: 0.6148743255270852\n",
      "[370/450] train loss: 0.6097013945917825\n",
      "[380/450] train loss: 0.6053637977493437\n",
      "[390/450] train loss: 0.604343974743134\n",
      "[400/450] train loss: 0.5991006511077285\n",
      "[410/450] train loss: 0.5967364082976085\n",
      "[420/450] train loss: 0.593176363053776\n",
      "[430/450] train loss: 0.5883588831438574\n",
      "[440/450] train loss: 0.5851710203696381\n",
      "[450/450] train loss: 0.5827775384320153\n",
      "val loss:  0.684599201977253\n",
      "val f1: {'f1': 0.81375}\n",
      "[10/450] train loss: 0.4832371920347214\n",
      "[20/450] train loss: 0.3796561874449253\n",
      "[30/450] train loss: 0.35923421680927276\n",
      "[40/450] train loss: 0.3955452099442482\n",
      "[50/450] train loss: 0.38769544899463654\n",
      "[60/450] train loss: 0.3810506929953893\n",
      "[70/450] train loss: 0.38711855305092674\n",
      "[80/450] train loss: 0.39033702462911607\n",
      "[90/450] train loss: 0.4031402806440989\n",
      "[100/450] train loss: 0.40563080832362175\n",
      "[110/450] train loss: 0.4066167643124407\n",
      "[120/450] train loss: 0.4183394281814496\n",
      "[130/450] train loss: 0.41473225160286975\n",
      "[140/450] train loss: 0.41364944374987056\n",
      "[150/450] train loss: 0.4100058575471242\n",
      "[160/450] train loss: 0.4158783437684178\n",
      "[170/450] train loss: 0.4059885499670225\n",
      "[180/450] train loss: 0.4048193388101127\n",
      "[190/450] train loss: 0.4029446960671952\n",
      "[200/450] train loss: 0.39794851187616587\n",
      "[210/450] train loss: 0.4029601019408022\n",
      "[220/450] train loss: 0.39624231088567863\n",
      "[230/450] train loss: 0.39549522740037546\n",
      "[240/450] train loss: 0.39457390466704967\n",
      "[250/450] train loss: 0.391558489471674\n",
      "[260/450] train loss: 0.3898175878306994\n",
      "[270/450] train loss: 0.3878052104126524\n",
      "[280/450] train loss: 0.38784262029720207\n",
      "[290/450] train loss: 0.38824283097838536\n",
      "[300/450] train loss: 0.3832691995302836\n",
      "[310/450] train loss: 0.3791009762114094\n",
      "[320/450] train loss: 0.3744220147607848\n",
      "[330/450] train loss: 0.3714709297951424\n",
      "[340/450] train loss: 0.37081120377954313\n",
      "[350/450] train loss: 0.37037022550191195\n",
      "[360/450] train loss: 0.3697777209803462\n",
      "[370/450] train loss: 0.366097199413422\n",
      "[380/450] train loss: 0.36338974811920993\n",
      "[390/450] train loss: 0.3639543895346996\n",
      "[400/450] train loss: 0.3612313138693571\n",
      "[410/450] train loss: 0.36116170766876965\n",
      "[420/450] train loss: 0.35855218067410444\n",
      "[430/450] train loss: 0.3566008836550768\n",
      "[440/450] train loss: 0.355160602571612\n",
      "[450/450] train loss: 0.35270642884903486\n",
      "val loss:  0.6933227476477622\n",
      "val f1: {'f1': 0.81875}\n",
      "[10/450] train loss: 0.3713035874068737\n",
      "[20/450] train loss: 0.27815129794180393\n",
      "[30/450] train loss: 0.256067530810833\n",
      "[40/450] train loss: 0.26658517383039\n",
      "[50/450] train loss: 0.24876421675086022\n",
      "[60/450] train loss: 0.24693939027686915\n",
      "[70/450] train loss: 0.24684686793812682\n",
      "[80/450] train loss: 0.24333634679205715\n",
      "[90/450] train loss: 0.2501453770117627\n",
      "[100/450] train loss: 0.25486086677759884\n",
      "[110/450] train loss: 0.25060230517251925\n",
      "[120/450] train loss: 0.25993808048466843\n",
      "[130/450] train loss: 0.25647898367964306\n",
      "[140/450] train loss: 0.25668354061033044\n",
      "[150/450] train loss: 0.2523261505365372\n",
      "[160/450] train loss: 0.2559493666049093\n",
      "[170/450] train loss: 0.24636087406645804\n",
      "[180/450] train loss: 0.2490781994536519\n",
      "[190/450] train loss: 0.2479401709413842\n",
      "[200/450] train loss: 0.24611489040777088\n",
      "[210/450] train loss: 0.24845783735315005\n",
      "[220/450] train loss: 0.24569158543917266\n",
      "[230/450] train loss: 0.24653274102703385\n",
      "[240/450] train loss: 0.24488576689424615\n",
      "[250/450] train loss: 0.24249805925786494\n",
      "[260/450] train loss: 0.2413736193942336\n",
      "[270/450] train loss: 0.24124150018173235\n",
      "[280/450] train loss: 0.2398389533428209\n",
      "[290/450] train loss: 0.23896612699689537\n",
      "[300/450] train loss: 0.23617134573558965\n",
      "[310/450] train loss: 0.23273283413821652\n",
      "[320/450] train loss: 0.2298983358312398\n",
      "[330/450] train loss: 0.227568959083521\n",
      "[340/450] train loss: 0.23091386662905708\n",
      "[350/450] train loss: 0.23061453823532377\n",
      "[360/450] train loss: 0.23085214181078806\n",
      "[370/450] train loss: 0.22747701733700326\n",
      "[380/450] train loss: 0.22578608354455545\n",
      "[390/450] train loss: 0.2274409427665747\n",
      "[400/450] train loss: 0.2262087962217629\n",
      "[410/450] train loss: 0.22706280694138714\n",
      "[420/450] train loss: 0.22624405027322825\n",
      "[430/450] train loss: 0.2244221207274254\n",
      "[440/450] train loss: 0.22219573058695954\n",
      "[450/450] train loss: 0.22187573730117746\n",
      "val loss:  0.7099682362377644\n",
      "val f1: {'f1': 0.8175}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=preprocessed_dataset[\"train\"].features[\"labels\"].num_classes)\n",
    "\n",
    "new_vocab_size = tokenizer.vocab_size + len(tokenizer.additional_special_tokens)\n",
    "model.resize_token_embeddings(new_vocab_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_model(model, preprocessed_dataset, optimizer, num_epochs=5, batch_size=16, cuda_device=CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
